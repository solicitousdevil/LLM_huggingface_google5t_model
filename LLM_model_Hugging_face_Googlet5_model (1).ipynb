{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xvVGnZcyyr40"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HuggingFace\n",
        "\n",
        "Hugging face provide two wrappers for LLM.\n",
        "1. Models hosted on HuggingFace Hub via API\n",
        "2. Local pipelines (download models locally).\n",
        "\n",
        "Both works with: text2text-generation, text-generation\n",
        "\n"
      ],
      "metadata": {
        "id": "dpL2NjDCW4KH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "R6ywo11wyfVn"
      },
      "outputs": [],
      "source": [
        "!pip -q install langchain huggingface_hub transformers sentence_transformers accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hugging face token from huggingface website"
      ],
      "metadata": {
        "id": "4yIirsU723x6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = 'hf_hurGoCzDHhmjCxLuoMgYuCFQxquMKTYTx'"
      ],
      "metadata": {
        "id": "_lxAum-Ry-Vd"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#imorting promptemplate from longchain, HuggingFaceHub, LLmchain"
      ],
      "metadata": {
        "id": "wrMMwXSo3Lvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
      ],
      "metadata": {
        "id": "A-fBEY9EzEnM"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the model from hugging face"
      ],
      "metadata": {
        "id": "T2OUgX793OOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(prompt=prompt,\n",
        "llm=HuggingFaceHub(repo_id=\"google/flan-t5-large\",\n",
        "                                        model_kwargs={\"temperature\":0,\n",
        "                                                      \"max_length\":100}))"
      ],
      "metadata": {
        "id": "eGkqzUmCzGBZ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q & A session"
      ],
      "metadata": {
        "id": "a2xOK-J_3SFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"how do agent to customers?\"\n",
        "\n",
        "print(llm_chain.run(question))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1h5M4owGzJE1",
        "outputId": "3e06c035-e951-4685-cc0c-4027f66c30d7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The customer is the person who pays for the services of the agent. The agent is the person who is responsible for the payment of the services of the agent. So the answer is the customer.\n"
          ]
        }
      ]
    }
  ]
}